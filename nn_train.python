import numpy as np
import warnings
from numba import jit
from numpy import arange

def get_data():
	data = np.genfromtxt("./data.csv", delimiter = ',')
	data_str = np.genfromtxt("./data.csv", delimiter = ',', dtype = np.str)
	x = data[:, 2:]
	y = np.zeros((len(data), 1))
	y[(data_str[:, 1] == 'M') == True] = 1
	return x, y

@jit
def sigmoid(x):
	return 1 / (1 + np.exp(-x))

@jit
def sigmoid_gradient(x):
	return sigmoid(x) * (1 - sigmoid(x))

def main():
	x, y = get_data()
	m = x.shape[0]
	topology = [30, 30, 20, 10, 1]
	theta = [np.random.randn(topology[1], topology[0] + 1)]
	for i in range (1, len(topology) - 1):
		theta.extend([np.random.randn(topology[i + 1], topology[i] + 1)])
	layers = len(topology) - 1
	epochs = 100000
	alpha = 0.00001
	lmbd = 0.00001
	for i in range(0, epochs + 1):
		## Feedforward
		a = [np.hstack((np.ones((m, 1)), x))]
		z = [np.dot(a[0], theta[0].T)]
		for j in range(0, layers - 1):
			a.extend([np.hstack((np.ones((len(z[j]), 1)), sigmoid(z[j])))])
			z.extend([np.dot(a[j + 1], theta[j + 1].T)])
		a.extend([sigmoid(z[layers - 1])])
		## Backpropagation
		d = [a[layers] - y]
		for j in range(0, layers - 1):
			d.extend([np.dot(d[j], theta[-(j + 1)][:, 1:]) * \
				sigmoid_gradient(z[-(j + 2)])])
		## Gradient Update
		for j in range(0, layers):
			theta_grad = np.dot(d[-(j + 1)].T, a[j]) / m
			theta_grad[:, 1:] += (lmbd * theta[j][:, 1:]) / m
			theta[j] -= alpha * theta_grad
		## Print Status
		true = 0
		false = 0
		for j in range(0, len(a[layers])):
			if (a[layers][j] >= 0.5 and y[j]) or (a[layers][j] < 0.5 and y[j] == 0):
				true += 1.
			if (a[layers][j] >= 0.5 and y[j] == 0) or (a[layers][j] < 0.5 and y[j]):
				false += 1.
		print("epoch {:d}/{:d} - loss: {:f} - cost: {:f}".format(i, \
			epochs, false / (true + false), \
			((np.dot(y.T, np.log(a[layers])) - \
			np.dot(1 - y.T, np.log(1 - a[layers]))) / m)[0][0]))

if __name__ == "__main__":
	warnings.simplefilter("ignore")
	np.random.seed(1)
	main()
